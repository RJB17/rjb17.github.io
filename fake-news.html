<!DOCTYPE html>
<html>

<head>
  <title>Rob Bowman</title>
  <link rel="stylesheet" href="./portfolio_styles.css">
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet">
  <link rel="icon" href="./icon.svg" sizes="any" type="image/svg+xml">
  <script src="./darp-script.js"></script>
  <meta name="viewport" content="width=device-width">
</head>

  <body onload="darp_load()">

  <header id="header">
    <div class='fixed-header'>

      <div class='logo-outer'>
        <div class='logo-inner'>
          <a href="./index.html">
            <img src="./robbowman.svg" class="logo">
          </a>
        </div>
      </div>

      <div id="header-nav" class="header-nav">
        <a class="nav-btn" href="./index.html">About</a>
        <a class="nav-btn current" href="./case-studies.html">Case Studies</a>
        <a class="nav-btn" href="./contact.html">Contact</a>
      </div>
  </header>

  <div id='content' class="content">
    <a class="back-btn" href="./case-studies.html">&#10094; All Case Studies</a>

    <h1> Case Study: Fake News </h1>

    <!-- Introduction -->
    <div class="content-panel">
      <div class="inner-content">
        <h2>Introduction</h2>
        <p>
          University team research project investigating interaction with
          tools for Fake News detection.
        </p>
        <p>
          The project involved reviewing the existing literature and then:
          designing, running, evaluating and reporting an experiment.
        </p>
        <p>
          The project started with being assigned to a 4 person team and given
          the topic of Fake News to research.
        </p>
        <p>
          I contributed significantly to most parts of the project described
          in this case study. I declare, when describing, the few parts of
          the project I did not contribute significantly to.
        </p>
        <p>
          Tags:
          <b>research</b>,
          <b>literature review</b>,
          <b>mixed-methods study</b>,
          <b>data analysis</b>,
          <b>statistical testing</b>.
        </p>
      </div>
    </div>

    <!-- Process -->
    <div class="content-panel">
      <div class="inner-content">
        <h2>Process</h2>
        <img
          src="./assets/fake-news/process.svg"
          alt=""
          class="process-svg center-img">
        <p>
          For this case study, the project shall be viewed as a sequence
          of 4 stages: literature review, experiment, results and discussion.
          Each stage shall be discussed in turn.
        </p>
      </div>
    </div>

    <!-- Literature Review -->
    <div class="content-panel">
      <div class=" inner-content">
        <h2>Literature Review</h2>
        <h3 class="header-start">Introduction</h3>
        <p>
          First, the existing literature was carefully reviewed.
          This involved: trawling the recent proceedings of
          reputable conferences, searching key terms, and
          following up the references of relevant publications.
          In the existing literature, the review aimed to identify:
        </p>
        <ul>
          <li>the research questions considered,</li>
          <li>the methods used,</li>
          <li>and the results found.</li>
        </ul>
        <p>
          The goal was to find a gap in the existing research
          with motivation to research it. Additionally, an understanding of
          the methods previously used would be used to inform the design of
          this study.
        </p>
        <p>
          A current, open and interesting research area identified was:
          <b>the design and use of tools to help users spot Fake News</b>.
        </p>
        <h3 class="header-follow">Motivation & Background</h3>
        <p>
          The literature review defined Fake News to be disinformation
          (intentionally false or misleading information) in the
          form of news. Fake News is considered harmful and people are highly
          susceptible to it due to multiple cognitive biases, thus, motivating
          the study of tools to help people spot it.
        </p>
        <h3 class="header-follow">Gap</h3>
        <p>
        Existing work found that:
        </p>
        <ol>
          <li>People are bad at spotting Fake News</li>
          <li>People think they are good at detecting Fake News</li>
          <li>People were sceptical of tools to help them detect Fake News</li>
        </ol>
        <p>
          <b>
            Existing work had <em>not</em> identified if, given tools to help
            them detect Fake News people would use the tools.
          </b>
          This project attempts to fill some of this gap.
        </p>
        <h3 class="header-follow">Context</h3>
        <p>
          The literature discussed how news consumption is changing with
          social media, eg. Facebook and Twitter, becoming an increasingly
          common way to consume news content. Consequently, <b>news consumption
          through social media was deemed an appropriate setting for this
          research</b>.
        </p>

        <h3 class="header-follow">Resulting Research Questions</h3>
        <p>
          The literature review provided motivation to research the question:
          <b>do credibility signals in social media news feeds affect users'
          perceptions of the truth of news?</b>
        </p>
        <p>
          Credibility signals are one form of tool for helping users spot Fake
          News. They present an estimated level of credibility such as low
          or high. They are a compromise between simpler tools like flags and
          more complex visualisations.
        </p>
        <p>
          A second research question identified was: <b>does the task of
          detecting Fake News affect peoples' perceptions of how good they are
          at it?</b>
        </p>
      </div>
    </div>

    <!-- Experiment -->
    <div class="content-panel">
      <div class="inner-content">
        <h2>Experiment</h2>
        <p>
          To investigate the research questions:
        </p>
        <ol>
          <li>
            Do credibility signals in social media news feeds affect users'
            perceptions of the truth of news?
          </li>
          <li>
            Does the task of detecting Fake News affect peoples' perceptions
            of how good they are at it?
          </li>
        </ol>
        <p>
          a mixed methods study was designed. The primary method was a rating
          exercise. It was complemented by questionnaires.
        </p>
        <h3 class="header-follow">Rating Exercise</h3>
        <p>
          The primary method was a rating exercise.
          <b>
            Participants were presented with a sequence of news articles and
            asked to rate how true they believed each article to be.
          </b>
          The articles were designed to appear similarly to how they would
          be presented in a newsfeed.
        </p>
        <p>
          The truth ratings were captured ussing a A 5-point
          Likert item with the values: values:
          ‘definitely false’, ‘probably false’, ‘unsure’, ‘probably true’,
          and ‘definitely true’.
          <b>The mean truth ratings given by participants
          for sequences of articles was the dependent variable (DV) of the
          study.
          </b>
        </p>
        <div class="articles">
          <h3 style="margin-bottom: 0">Example Articles</h3>
          <div>
            <div class="article">
              <img
                src="./assets/fake-news/N5H.jpg"
                alt=""
                class="center-img"
              >
            </div>
            <div class="article">
              <img
                src="./assets/fake-news/N1L.jpg"
                alt=""
                class="center-img"

              >
            </div>
          </div>

          <div class="slide-nav">
            <div class="slide-btns">
              <a class="prev" onclick="plusArticles(-1)">&#10094;</a>
              <a class='article-btn' onclick="currentArticle(1)"></a>
              <a class='article-btn' onclick="currentArticle(2)"></a>
              <a class="next" onclick="plusArticles(1)">&#10095;</a>
            </div>
          </div>
        </div>
        <p>
          Every article was presented with a credibility signal of either low
          or high value. <b>The levels of the credibility signal formed one
          independent variable (IV) of the study</b>.
        </p>
        <p>
          Effects of showing certain articles with certain signals were
          counterbalanced by partitioning the participant into two groups
          such that for each article, the groups
          saw it with different signals. <b>The group partition
          formed the second independent variable (IV) of the study</b>.
        </p>
        <p>
          This resulted in a 2x2
          mixed design, thus, four conditions as summarised by Tables 1 & 2.
        </p>
        <img
          src="./assets/fake-news/table1.png"
          alt=""
          class="center-img table1"
        >
        <img
          src="./assets/fake-news/table2.png"
          alt=""
          class="center-img table2"
        >
        <h3 class="header-start">Questionnaires</h3>
        <p>
          The rating exercise was complemented by pre- and post-questionnaires.
          The questionnaires had a few different purposes.
        </p>
        <p>
          First the questionnaires were used
          to collect <b>socio-demographic information</b>
          to enable an understanding of
          who participated in the study. This understanding is important for
          evaluating the validity of the study.
        </p>
        <p>
          Second, the questionnaires gathered
          <b>information relating to potential confounds </b>.
          The potential confounds include:
        </p>
        <ul>
          <li> the social media the participants used</li>
          <li>
            their familiarity with the articles presented in the
            rating exercise
          </li>
        </ul>
        <p>
          Again this is important for assessing the
          studies validity. </p>
        <p>
          The post-questionnaire explored
          <b>why participants gave the truth ratings they did</b>
          as well as
          <b> how influenced they felt by the signals</b>
          and
          <b>how believable they thought the signals were</b>.
          This
          information was gathered as it had the potential to help us
          explain the results we collected.
        </p>
        <p>
          Finally, in both the pre- and
          post-questionnaire participants were asked
          <b>how good they thought they were at detecting Fake News</b>.
          This was asked, repeatedly, to identify if the rating exercise
          decreases self-efficacy.
        </p>
        <h3 class="header-follow">Hypothesis</h3>
        <p>
          We believed that participants would be affected by the signals and
          hypothesised that:
          <b>
            high credibility signals in social media news feeds
            will lead users to perceive the article as being more truthful
          </b>.
          This hypothesis relates to the study's first research question.
        </p>
        <p>
          The existing literature reported people to perceive themselves as
          being good at detecting Fake News but bad in reality. We suspected
          that the task of detecting Fake News may serve as a reality check,
          thus, hypothesised that:
          <b>
            the task of rating how true news articles
            are will decrease peoples self efficacy at Fake News detection
          </b>.
          This hypothesis relates to the study's second research question.
        </p>
        <h3 class="header-follow">Running the study</h3>
        <p>
          The study was implemented using <b>Qualtrics</b>.
          First we piloted the study
          with a small number of participants to identify problems with the
          study and to modify it as necessary. The pilot indicated the study
          was longer than targeted so modifications were made to shorten it.
          <b>60 student participants</b>
          were recruited by approaching them in communal
          areas of the university such as the library. I was not involved in
          participant recruitment.
        </p>
        <p>
          At the start of every session with a participant they were briefed
          about the study and it was explained how their data would be used such
          that they could give informed consent. They were able to withdraw,
          without giving reason, during the experiment. At the end of the study
          they were debriefed and able to ask any questions.
        </p>
      </div>
    </div>

    <!-- Results -->
    <div class="content-panel">
      <div class="inner-content">
        <h2>Results</h2>
        <p>
          The experiment collected a large amount of data. The collected data
          could be downloaded from Qualtrics as a large spreadsheet.
          <b>
            Using Python, largely the Pandas library, I processed the data
            to produce well formatted data files for each part of the analysis.
          </b>
        </p>
        <p>
          Using R, a team mate and I,
          <b>
            explored each of the data sets using
            descriptive statistics and data visualisation.
          </b>
          We used appropriate statistical tests to test the hypothesis.
        </p>
        <h3 class="header-follow">Hypothesis 1</h3>
        <p>
          The first hypothesis was: high credibility signals in social media
          news feeds will lead users to perceive the article as being more
          truthful.
        </p>
        <p>
          To test this hypothesis we wanted to
          <b>
            compare the mean truth ratings of articles with low credibility
            and articles with high credibility.
          </b>
          If the hypothesis is correct we would hope to
          to see a difference in location i.e. we would expect the mean of the
          high credibility level to be greater than the mean of the low
          credibility level.
        </p>
        <img
          src="./assets/fake-news/truth-ratings.png"
          alt=""
          class="center-img truth-rating-img"
        >
        <p>
          Given the 2x2 mixed design of the experiment, the appropriate
          statistical test was a <b>2x2 mixed ANOVA.</b>
          This ANOVA is an omnibus test that examines 3 possible effects.
        </p>
        <p>
          The first effect is a
          <b>main effect for the Credibility Signal IV</b>.
          This is the effect changing the level of the Credibility Signal
          has on the Mean Truth Rating DV. The hypothesis predicts an
          effect for the Credibility Signal, however, the ANOVA showed there
          was <b>not a statistically significant</b> one.
          F(1,56)=2.726, p=.104, η<sup>2</sup>=.046.
          This can be seen visually from
          the boxplot above - the high signal conditions (A.high & B.high)
          were not rated notably more true then the low signal conditions
          (A.low & B.low).
        </p>
        <p>
          The second effect is a <b>main effect for the Group IV</b>.
          This is the
          effect of changing the Group IV on the Truth Rating DV. No effect is
          expected here. Surprisingly, the ANOVA reported a <b>significant and
          large effect</b>.
          F(1,56)=8.520, p=.005, partial η<sup>2</sup>=.132
          This indicates that
          Group B thought the articles were more likely true than Group A.
        </p>
        <p>
          The final effect was the <b>interaction effect</b>.
          This effect that the
          combination of the Credibility Signal IV and the Group IV has on
          the DV. We expect the IVs to be independent, hence, we do not expect
          there to be an effect. As expected, the ANOVA <b>did not find a
          significant interaction effect</b>.
          F(1,56)=0.041, p=.839, η<sup>2</sup>=.001
        </p>
        <p>
          <b>
            In summary, the experiment did not find the result
            hypothesised.
          </b>
          The hypothesised result was that articles with high credibility
          signals would be perceived as more truthful than articles with
          low credibility signals. Therefore, this experiment provides
          evidence against the hypothesis. Additionally, the experiment
          found the unexpected result that the Group IV affects perceived
          truth in a statistically significant way. The unexpected result
          shall be discussed in the discussion section of this case study.
        </p>

        <h3 class="header-follow">Hypothesis 2</h3>
        <p>
          Hypothesis 2 was: the task of rating how true news articles
          are will decrease peoples self-efficacy at Fake News detection
        </p>
        <p>
          <b>
            Repeat measures of participants’ self-efficacy at Fake News detection
            were taken. The first measure was before the rating exercise and
            the second after.
          </b>
          A 5-point Likert item was used with the values: very bad, bad,
          average, good and very good.
          <b>
            Change in self-efficacy was computed by deducting the first
            measure from the second per participant.
          </b>
        </p>
        <p>
          The boxplot below visually describes the set of self-efficacy
          changes recorded. It is clearly
          <b>skewed to negative values showing a decrease in self-efficacy.</b>
        </p>
        <img
          src="./assets/fake-news/change.png"
          alt=""
          class="center-img self-efficacy-img"
          >
        <p>
          To test the hypothesis formally, a <b>Wilcoxon signed-rank test</b>
          was used.
          The test reported T(30) = 55, p<.001. This means that there was a
          <b>
            statistically significant change in self-efficacy which supports
            the hypothesis.
          </b>
        </p>


        <h3 class="header-follow">Credibility Signal Influence</h3>
        <img
          src="./assets/fake-news/influence.png"
          alt=""
          class="center-img bi-plot">
        <p>
          In the post-questionnaire participants were asked how influenced
          they thought they were by the credibility signals. The
          participants’ responses were collected using a 4 point Likert item.
          This was of interest as it is highly relevant to the study’s first
          research question. The first research question considered if
          people would be influenced by credibility signals and this
          questionnaire question asks if the participants felt they were.
        </p>
        <p>
          <b>
            It was found that almost three quarters of participants felt that
            they were at least slightly influenced by the credibility signals.
          </b>
        </p>

        <h3 class="header-follow">Crediblity Signal Believability</h3>
        <img
          src="./assets/fake-news/believability.png"
          alt=""
          class="center-img bi-plot">
        <p>
          The existing literature had reported that people were sceptical of
          tools for helping them detect Fake News. Therefore, when considering
          if the signals affect people it is interesting to investigate if
          people considered the signals believable. People may be too
          sceptical of tools for Fake News detection to find any tool
          believable. Additionally, the design of the tool may affect
          its believability, hence, participants not being affected by
          the signals may be related to the believability of these
          specific signals.
        </p>
        <p>
          In the post-questionnaire, participants were asked how believable
          they found the credibility signals and their response was
          collected using a 4 point Likert item.
        </p>
        <p>
          The results looked very similar to those collected for the
          question that asked participants how influenced they thought
          they were by the signals.
          <b>
            About three quarters of participants
            thought the signals were at least slightly believable.
          </b>
        </p>

        <h3 class="header-follow">Further Analysis</h3>
        <p>
          The post-questionnaire was used to collect data about why
          participants gave the truth ratings they did. For each article they
          had been shown during the rating exercise, they were shown the
          article again along with the rating they gavegave, and asked
          for the main reasons they gave that rating. The responses were
          collected using an open text field. The data collected was analysed
          using content analysis, however, I did not contribute substantially
          to the content analysis so shall not discuss it further.
        </p>

      </div>
    </div>

    <!--Discusssion  -->
    <div class="content-panel">
      <div class="inner-content">
        <h2>Discussion</h2>
        <p>The study aimed to test two hypotheses..</p>
        <p>
          The first hypothesis was that: high credibility signals in social
          media news feeds will lead users to perceive the article as being
          more truthful. This hypothesis was tested using the rating
          exercise component of the experiment.
          <b>
            The experiment did not find a statistically significant effect
            of credibility signals on the perceived truth of articles.
          </b>
          This fits with the existing literature that reported people
          didn't think they would use credibility signals because they
          consider themselves good at detecting Fake News and are dubious of
          tools to help them. The participants did not consider the signals
          used in this study to be very believable; this lack of believability
          could be a contributing factor.
        </p>
        <p>
          The second hypothesis was that: the task of rating how true news
          articles are will decrease peoples self efficacy at Fake News
          detection. This hypothesis was tested by comparing peoples
          self-efficacy before and after the rating exercise.
          <b>
            A statistically significant effect was found of the rating
            exercise of Fake News detection self-efficacy.
          </b>
          Our interpretation of this was that the task acts as a reality
          check causing the participant to readjust their perception of
          their ability. Consequently, an exercise like the rating exercise
          used in the experiment has the potential to be used as a tool to
          train people against falling for Fake News.
        </p>
        <p>
          Additionally,
          <b>
            an unexpected statistically significant effect of the group the
            participant was in on how true they perceived articles to be
            was found.
          </b>
          This result suggests that participants in one group perceived
          articles to be considerably more truthful than participants in
          the other group did. This result seems odd. We carefully examined
          the experiment design and data in search of a cause but did not
          conclude on one. The only notable difference between the groups
          was their gender ratios. One group had a female:male ratio of 3:4
          and the other 1:2. It's possible there is a relationship between
          gender and how truthful news is perceived to be. It is also
          possible that this is simply a false positive. Of course it
          is also possible there was a problem we did not identify, however,
          we did examine the design thoroughly and consult our course
          instructors who are experienced researchers to minimise
          this possibility. Further work would be required to further
          our understanding of this result.
        </p>

      </div>
    </div>

    <!-- Conclusion  -->
    <div class="content-panel">
      <div class="inner-content">
        <h2>Conclusion</h2>
      </div>
    </div>

  <a class="back-btn" href="./case-studies.html">&#10094; All Case Studies</a>
  </div>


  <footer id="footer">
    <div class="fixed-footer">
      <div class="footer-nav">
        <a class="nav-btn" href="./index.html">About</a>
        <a class="nav-btn current" href="./case-studies.html">Case Studies</a>
        <a class="nav-btn" href="./contact.html">Contact</a>
      </div>
    </div>
  </footer>

</body>

</html>
